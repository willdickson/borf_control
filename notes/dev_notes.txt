dev_notes.txt - borf_control development notes

Will Dickson 06/12/2008 - 
=======================================================================
 
Stability problem. 
==================

The current incarnation of borf_control has a pretty serious stability
issue which causes it to hard lock intermittently. I'm not sure what
is causing this - it may have something to do with shared memory
usage. This set of notes has been started to help me keep track of my
debugging effort.

A couple of ideas. 

  I think the stability problem has to due with shared memory as the
  problem typically seems to occur when access or freeing the shared
  memory between the realtime driver and the user space library. 

  One thought is that I need to use mlock on the shared memory buffers.
  
  A second idea is that it is a race condition. I might try commenting
  out all spin locks in motor_ctl driver. 

------------------------------------------------------------------------

* Found that rt_global_heap_open() is required for user space
  processes using shared memory.

  Action: 
  
  In motor_shm.c I've added a rt_global_heap_open() to the section of
  code just after the rt_shm_alloc's and a rt_global_heap_close() the
  the section just after the rt_shm_free's.

  Result:

  This didn't seem to fix the problem and stress.py still hard locks
  the system after running about 20-40 kinematic patterns. The final
  command on the debugging output was *cmd2motor_ctl set-os-buffer.
  I'm going to check this out.  

  In addition because of the hard locks I should back up all the debs
  required to make this system. and all the src packages.

  Note, I'm going to keep the rt_global_heap open and close functions
  in motor_shm.c as the rtai documentation and example seem to suggest
  they are required. 


-------------------------------------------------------------------------

* Maybe I need to lock my shared memory buffers in use space

  Action:
  
  In motor_shm.c try adding a mlock to each of the shared memory
  buffers after allocation and a munlock before freeing them.
   
  Result:
  
  Hard-lock on 73rd kinematic pattren using stress.py. Last debug
  command was cmd2motor_ctl unlock-buffer. Again it is unclear why
  this is occuring. Examine motor_ctl.c code for fifo command and fifo
  code in motor_comm. Could the fifo be the problem? What about
  locking the memory in the kernel modules? is that necessary?

  Note, I'm going to keep the mlocks in the code as they don't seem to
  hurt anything. 

------------------------------------------------------------------------

* Race condition?

  Could there be a race condition in motor_ctl.c? I can't see anything
  obvious. But I'm making pretty heavy use of spinlocks which could be
  bad.

  Action: 
  
  Try removing all spinlocks and see what happens. This could mess up
  the behavior of the system in and of itself, but it should not get
  into a race condition -  it just might be that some of the variables
  are garbage when they are accessed though. 

  Result:

  Made it through stress.py w/ number of tests = 200. I think (hope)
  that this was the problem. It appears that that maybe I did have a
  race condition through my use of spinlocks. I'll need to rexamine
  motor_ctl.c and see if I can find another way to deal with the
  concurrancy issues.
  
-------------------------------------------------------------------------

* Assuming that the stability problem was a race condition I now need
  to figure out the best way to deal with it. Need to look through the
  real time code and figure out what the concurrancy issues are and
  how best to deal with them. RTAI offers a host of mechanisms for
  dealing with concurrancy - I'll need to spend a little time looking
  through them to figure out what might be the best approach. 


  What shared data structures do we have, who shares them, and how are
  they shared?
  
  Shared structures: sys_state, mv_buffer, os_buffer, trig_state

  Since the shared use of mv_buffer, os_buffer ans trig_state is
  limited. I'm going to focus on the sys_state data structure. It is
  used by both the RT task (we only have one of these) and the command
  fifo handler cmd_handler which is a linux kernel task. Because
  cmd_handler is a linux kernel task, it can never interrupt execution
  the RT task (when not sleeping), however the RT task could and
  probably does preempt the cmd_handler on occasion. We need to ensure
  - without breaking realtime - that the RT task alway has a consitent
  view of sys_state.

  Overview of sys_state structure and shared items
  
    sys_state fields                       sharing
    ------------------------------------------------------------
    
                                       RT task          cmd_handler
                                                   
    RT_TASK main_taks              |      r        |        -
    int outscan                    |      rw       |        rw
    int standby                    |      r        |        rw
    int enable                     |      r        |        rw
    int buffer_lock                |      r        |        rw
    int buffer_pos                 |      rw       |        rw
    int motor_index[NUM_MOTOR]	   |      rw       |        -
    int motor_type[NUM_MOTOR]      |      r        |        r
    int loop_mode                  |      r        |        rw
    int buffer                     |      r        |        w
    comedi_t *device               |      r        |        -
    lsampl_t ain_data[NUM_AIN]     |      rw       |        -


  We have a number of fields which are shared between the RT task and
  the cmd_handler. One thought is to eliminate some of this sharing by
  moving the functionality over to th RT task. Many of the fields are
  just integers (and could be single bits). 




==========================================================================

Other ideas -- May not be needed.
--------------------------------------------------------------------------

* Is it a shared memory access the problem?

  Action: 

  Try removing alla writes from user space to shared memory.

  Result:

-------------------------------------------------------------------------


